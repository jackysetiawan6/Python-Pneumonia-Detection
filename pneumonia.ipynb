{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0. Project Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Owner      : Jacky Setiawan (2602190444) and Ichsan Ilyasa (2602191245)\n",
    "# Date       : Monday, May 6th 2024\n",
    "# Description: This is a machine learning project for research methodology course\n",
    "# Project    : Deep Learning Modelling for Pneumonia Detection using Chest Radiographs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Dataset Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Source     : https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n",
    "# Dataset Description: This dataset was collected from Guangzhou Women and Childrenâ€™s Medical Center's paper named \"Detecting Pneumonia in Chest X-Rays with a CNN\"\n",
    "# Dataset Content    : The dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (Pneumonia/Normal)\n",
    "# Dataset Features   : All chest radiographs were initially screened for quality control by removing all low quality or unreadable scans\n",
    "# Dataset Overview   : 5863 instances, 2 categories (Pneumonia/Normal)\n",
    "\n",
    "# Add Ons            : https://data.mendeley.com/datasets/rscbjbr9sj/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Dataset Collection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization, Input, Concatenate, LSTM\n",
    "from keras.models import Sequential, Model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from ultralytics import YOLO\n",
    "from xgboost import XGBClassifier as xgb, DMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['NORMAL', 'PNEUMONIA']\n",
    "\n",
    "def get_training_data(data_dir):\n",
    "    X, y = [], []\n",
    "    for label in labels:\n",
    "        path = os.path.join(data_dir, label)\n",
    "        class_num = labels.index(label)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                resized_arr = cv2.resize(img_arr, (150, 150))\n",
    "                X.append(resized_arr)\n",
    "                y.append(class_num)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = get_training_data('pneumonia/train')\n",
    "X_test, y_test = get_training_data('pneumonia/test')\n",
    "X_val, y_val = get_training_data('pneumonia/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "pd.DataFrame(y_train).value_counts().plot(kind='bar', color=['gray', 'gray'], title='Pneumonia vs Normal')\n",
    "plt.xticks(range(len(labels)), labels, rotation=0)\n",
    "plt.xlabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(X_train[0], cmap='gray')\n",
    "plt.title(labels[y_train[0]])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(X_train[-1], cmap='gray')\n",
    "plt.title(labels[y_train[-1]])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = (np.array(X_train) / 255).reshape(-1, 150, 150, 1)\n",
    "X_test = (np.array(X_test) / 255).reshape(-1, 150, 150, 1)\n",
    "X_val = (np.array(X_val) / 255).reshape(-1, 150, 150, 1)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(featurewise_center = False, samplewise_center = False,\n",
    "                             featurewise_std_normalization = False, samplewise_std_normalization = False,\n",
    "                             zca_whitening = False, rotation_range = 30, zoom_range = 0.2,\n",
    "                             width_shift_range = 0.1, height_shift_range = 0.1, horizontal_flip = True,\n",
    "                             vertical_flip = False)\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Convolutional Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(150, 150, 1)))\n",
    "model.add(Conv2D(32, (3, 3), strides=1, padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), strides=1, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), strides=1, padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), strides=1, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), strides=1, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(optimizer=\"rmsprop\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1, factor=0.3, min_lr=0.00001)\n",
    "history = model.fit(datagen.flow(X_train,y_train, batch_size = 16), epochs = 100, validation_data = datagen.flow(X_val, y_val), callbacks = [learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model.evaluate(X_test, y_test)\n",
    "print(\"Loss of the model is - \" , evaluation[0])\n",
    "print(\"Accuracy of the model is - \" , evaluation[1]*100 , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [i for i in range(100)]\n",
    "fig , ax = plt.subplots(1,2)\n",
    "train_acc = history.history['accuracy']\n",
    "train_loss = history.history['loss']\n",
    "val_acc = history.history['val_accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "ax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\n",
    "ax[0].plot(epochs , val_acc , 'ro-' , label = 'Validation Accuracy')\n",
    "ax[0].set_title('Training & Validation Accuracy')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "ax[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "ax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')\n",
    "ax[1].plot(epochs , val_loss , 'r-o' , label = 'Validation Loss')\n",
    "ax[1].set_title('Testing Accuracy & Loss')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "ax[1].set_ylabel(\"Training & Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.round(y_pred)\n",
    "\n",
    "classification_report = classification_report(y_test, y_pred_classes)\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('pneumonia_models/model-cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Visual Geometry Group 19**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (150, 150, 1)\n",
    "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "x = Concatenate()([input_layer, input_layer, input_layer])\n",
    "x = base_model(x, training=False)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1, factor=0.3, min_lr=0.00001)\n",
    "history = model.fit(datagen.flow(X_train,y_train, batch_size = 16), epochs = 100, validation_data = datagen.flow(X_val, y_val), callbacks = [learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model.evaluate(X_test, y_test)\n",
    "print(\"Loss of the model is - \" , evaluation[0])\n",
    "print(\"Accuracy of the model is - \" , evaluation[1]*100 , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [i for i in range(100)]\n",
    "fig , ax = plt.subplots(1,2)\n",
    "train_acc = history.history['accuracy']\n",
    "train_loss = history.history['loss']\n",
    "val_acc = history.history['val_accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "ax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\n",
    "ax[0].plot(epochs , val_acc , 'ro-' , label = 'Validation Accuracy')\n",
    "ax[0].set_title('Training & Validation Accuracy')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "ax[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "ax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')\n",
    "ax[1].plot(epochs , val_loss , 'r-o' , label = 'Validation Loss')\n",
    "ax[1].set_title('Testing Accuracy & Loss')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "ax[1].set_ylabel(\"Training & Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "if y_test.ndim > 1:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "report = classification_report(y_test, y_pred_classes)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('pneumonia_models/model-vgg.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. eXtreme Gradient Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "\n",
    "X_train_combined = np.vstack((X_train_flat, X_val_flat))\n",
    "y_train_combined = np.concatenate((y_train, y_val))\n",
    "\n",
    "model = xgb(\n",
    "    max_depth=6,\n",
    "    learning_rate=0.0001,\n",
    "    n_estimators=100,\n",
    "    eval_metric='logloss',\n",
    "    n_jobs=-1,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "model.fit(X_train_combined, y_train_combined,\n",
    "          eval_set=[(X_test_flat, y_test)],\n",
    "          verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_flat)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Long Short Term Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(150, 150), return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=RMSprop(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1, factor=0.3, min_lr=0.00001)\n",
    "history = model.fit(datagen.flow(X_train,y_train, batch_size = 16), epochs = 50, validation_data = datagen.flow(X_val, y_val), callbacks = [learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "\n",
    "classification_report = classification_report(y_test, y_pred_classes)\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7. Multi Layer Perceptron**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_flat, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val_flat, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_flat, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(150 * 150, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256, 2)\n",
    ")\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_pred, y_true = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "report = classification_report(y_true, y_pred, target_names=['NORMAL', 'PNEUMONIA'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **8. You Only Look Once**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n-cls.pt')\n",
    "\n",
    "results = model.train(\n",
    "    data='pneumonia',\n",
    "    epochs=100,\n",
    "    imgsz=150,\n",
    "    batch=16,\n",
    "    lr0=0.0001,\n",
    "    pretrained=True\n",
    ")\n",
    "\n",
    "results_val = model.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(source='pneumonia_yolo/test', save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = [result.probs.top1 for result in results]\n",
    "true_labels = []\n",
    "\n",
    "for result in results:\n",
    "    filename = result.path.split('\\\\')[-1]\n",
    "    if filename.startswith('person'):\n",
    "        true_labels.append(1)\n",
    "    else:\n",
    "        true_labels.append(0)\n",
    "\n",
    "report = classification_report(true_labels, pred_labels)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
